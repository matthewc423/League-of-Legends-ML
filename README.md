Matthew Chung

Predicting the rank of League of Legends games
https://youtu.be/kgE6AvFcrDE

In this project, I wanted to use machine learning in order to be able to predict the rank of a match when given its data. To start off, I needed to obtain a dataset, so I used datacollection.py (or .ipynb) in order to extract all the information I needed. If you wish to run the datacollection file (which I highly recommend not doing since it takes an extremely long time), you would first need to install the riotwatcher library and get access to a riot api key, which requires a riot account. However, the key must be regenerated every 24 hours and will stop working if you do not do so. This key will allow you to access the riot api freely with the help of riotwatcher, but will be limited to 100 requests every 2 minutes. The variables 'my_region' and 'region' represent which region we are looking at for matches, which is North America. The reason why there are two region variables is because the riot api is inconsistent and uses different region names for different requests. For example, finding matches will use 'americas' as the region while getting summoner information will use 'na1' as the region. The next variable underneath is the version, which changes every time League of Legends is updated. Currently they are on patch 11.23.1, but soon they may update to 11.24.1. The next line is a test request to the api that gets my summoner information after inputting my summoner name. Usually, I use this to see whether the api is still working, if my key has expired, or anything else that might be going wrong with the riot servers. If all of these things check out, you can run all the code. The matchdata is retrieved by tier and all the data is output to a csv that corresponds to the tier of the data. Once again, this may take hours to run so I have a ready-made data file that you can use for the next parts.

The next part is the actual machine learning that was done with the data. The first machine learning file I made was the training.py (or .ipynb) file, which uses scikit's linear regression model and mlp regressor model. Before running any of the code, you must make sure that the data.csv file is in the same directory as the training file. In the line df = df.drop(["objectivesStolen", "objectivesStolenAssists"], axis=1), we are getting rid of certain columns from data.csv that don't seem too useful. If you wish to drop other columns or keep the columns that are being removed, just add or remove the column headers into the list in the df.drop line in any order, then rerun the entire program. After this point just running the code will do everything for you and will output the results at the bottom. 

The last part is similar to the second part, except that this part uses keras and tensorflow instead. Once again, the data.csv file must be in the same directory. Again, you can get rid of certain columns in the line X = data.drop(["matchElo"], axis=1) by adding or removing columns in the list within the drop function, but this time you cannot remove "matchElo" no matter what since it's the dependent variable and cannot be with the independent variables. If you do choose to remove columns, you must go to the part where the creation of the sequential model takes place and change the input_dim to reflect the number of inputs you now have. For example, if no independent variables are removed, the input_dim = 22. From here you can just run the entire program. At the bottom of the program, you can repeatedly run the last line once the model is created to see how close individual tests get to their true values. 

All of this was done on google colab, so the data.csv file was put into the same directory by dragging it into the file section. 